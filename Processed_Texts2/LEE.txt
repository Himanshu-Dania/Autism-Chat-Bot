sensors Letter DeepLearningBased Detection of Infants with Autism Spectrum Disorder Using AutoEncoder Feature Representation Jung Hyuk Lee Geon Woo Lee Guiyoung Bong Hee Jeong Yooand Hong Kook Kim School of Electrical Engineering and Computer Science Gwangju Institute of Science and Technology Gwangju Korea JHL GWL Department of Psychiatry Seoul National University Bundang Hospital Seongnamsi Gyeonggido Korea GB HJY Department of Psychiatry College of Medicine Seoul National University Seoul Korea Correspondence Received October Accepted November Published November gidgidgidgidgidgidgidgidgid gid gidgidgidgidgidgidgid Abstract Autism spectrum disorder ASD is a developmental disorder with a lifespan disability While diagnostic instruments have been developed and qualied based on the accuracy of the discrimination of children with ASD from typical development TD children the stability of such procedures can be disrupted by limitations pertaining to time expenses and the subjectivity of clinicians Consequently automated diagnostic methods have been developed for acquiring objective measures of autism and in various elds of research vocal characteristics have not only been reported as distinctive characteristics by clinicians but have also shown promising performance in several studies utilizing deep learning models based on the automated discrimination of children with ASD from children with TD However di culties still exist in terms of the characteristics of the data the complexity of the analysis and the lack of arranged data caused by the low accessibility for diagnosis and the need to secure anonymity In order to address these issues we introduce a pretrained feature extraction autoencoder model and a joint optimization scheme which can achieve robustness for widely distributed and unrened data using a deeplearningbased method for the detection of autism that utilizes various models By adopting this autoencoderbased feature extraction and joint optimization in the extended version of the Geneva minimalistic acoustic parameter set eGeMAPS speech feature data set we acquire improved performance in the detection of ASD in infants compared to the raw data set Keywords autoencoder bidirectional long shortterm memory BLSTM joint optimization acoustic feature extraction autism spectrum disorder Introduction Autism spectrum disorder ASD is a developmental disorder with a high probability of causing diculties in social interactions with other people According to the Diagnostic and Statistical Manual of Mental Disorders Fifth Edition DSM ASD involves several characteristics such as being conned to specic interests or behaviors delayed linguistic development and poor functionality in terms of communicating or functioning in social situations As there is wide variation in terms of the types and severities of ASD based on its characteristics the disorder is referred to as a spectrum Not only does ASD have the characteristics of a developmental disorder with a lifespan disability but its prevalence is also increasingfrom in children in to in children in As diverse evidence has been obtained from previous research showing that the chance of improvement in the Sensors doi s journal sensorsSensors of social abilities of people with ASD increases when an earlier clinical intervention is performed the early detection of ASD characteristics has become a key point of current ASD research Various instruments for discriminating ASD have been developed and the commonly accepted gold standard schemes are behavioral assessments which are timeconsuming procedures and require multidisciplinary teams MDTs However most behavioral assessments su er in terms of the stability of their ASD diagnosis as a result of the issues of accessibility or subjectivity and interpretive bias between professions Therefore several attempts to develop objective and precise diagnostic methods have been made in multiple elds such as genetic determination principle analysis of brain images and physiological approaches One prominent area of behavioral observations is that of infants vocal characteristics Children with ASD are known to have abnormalities in their prosody resulting from decits in their ability to recognize the inherent mental conditions of others and their atypical vocalizations are known to be monotonous or exaggerated which can be revealed using various acoustic characteristics followed by engineering approaches for the discrimination of ASD or typical development TD in children based on the vocal and acoustic features For example in the researchers estimated decits in the vocalization of children with ASD at an average age of months such as at intonation atypical pitch or control of volume based on the variability of pitch and the longterm average spectrum LTAS using fast Fourier transform where signicant di erences were observed in the spectral components at lowband frequencies as well as spectral peaks and larger pitch ranges and standard deviations The development of linguistic abilities is also considered to be a distinguishable feature of delayed development in children with ASD Earlier vocal patterns at age months were proven to be di erentiable in a study that aimed to conrm the hypothetical vocal patterns and social quality of vocal behavior in order to di erentiate between ASD and TD cohorts in groups of children aged and months in terms of categorized speech patterns consisting of vocalization long reduplicated babbling twosyllable babbling and rst words Evidence of abnormalities in children with ASD were shown in these cases as a signicant decrease in vocalization and rst word rate while the di erence in babbling ability between children with ASD and TD was negligible Given the development and improvement of machine learning algorithms as the achievement in the performance of stateoftheart classication and discrimination tasks recent attempts to develop automated classication methods based on machine learning techniques have been based on the distinctiveness of vocal characteristics and have been shown to be promising alternatives to the conventional methods in many publications For examples of machine learning classication the researchers of employed various acousticprosodic features including fundamental frequency formant frequencies harmonics and root mean square signal energy In their research support vector machines SVMs and probabilistic neural networks PNNs were adopted as classiers which showed e ectual accuracy in discriminating children with ASD from children with TD Meanwhile the authors of employed more recent deep learning techniques such as convolutional neural networks CNNs and recurrent neural networks RNNs with spectral features from shorttime Fourier transform STFT and constant Q transform CQT to classify children diagnosed using the autism diagnostic observation schedule ADOS also showing promising results in multiple outcomes from SVMs RNNs and a combination of CNN and RNN classiers A generalized acoustic feature set an extended version of the Geneva minimalistic acoustic parameter set eGeMAPS and the bidirectional long shortterm memory BLSTM model were adopted to di erentiate between children with ASD and children with TD in showing that of the subjects utterances were correctly classied with the simple application of a deep learning model and feature sets While the quality of previous research based on various acoustic features has proven the e ectiveness of acoustic features and classication algorithms for the detection of abnormalities in childrens voices in ASD group compared to those of TD group the complexity and relationship being inherent between the features will remain uncertain until a large amount of data can be accumulated Furthermore a limitation still remains in terms of the problems regarding data collection since there areSensors of diculties pertaining to the need to secure the anonymity of infant subjects as well as the unintended ignorance of parents at earlier stages of their infants development The data of infants are accordingly dispersed by gender age and number of vocalizations or consist of comparably small volumes of audio engineering data in general These problems were typically overlooked by previous research with controlled and small amounts of data In order to provide suggestions for a method to overcome the abovementioned restrictions we focus on examining the feasibility of neural networks as a feature extractor employing an autoencoder AE which can modify acoustic features into lowered and separable feature dimensions We construct a simple sixlayered stacked AE that contains an input layer three fully connected FC layers an output layer and one auxiliary output layer which has categorical targets for ASD and TD for the optimization of the latent feature space of the AE We train the AE and deep learning models and compare the results for each model based on SVMs and vanilla BLSTM while adopting the same model parameters from the method suggested in The remainder of this paper is organized as follows Section describes the specications of the participants data data processing feature extraction statistical analysis and experimental setup Section presents the performance evaluations for each algorithm of the SVMs and vanilla BLSTM Lastly Section concludes the paper Proposed Method Data Collection and Acoustic Feature Extraction This study was based on the audio data from video recordings of ASD diagnoses which were collected from to at Seoul National University Bundang Hospital SNUBH We received approval from the Institutional Review Board IRB at SNUBH to use fully anonymized data for retrospective analysis IRB no B from existing research IRB no B We collected the audio data of infants who were assessed using seven multiple instruments consisting of ADOS second edition ADOS the autism diagnostic interview revised ADIR the behavior development screening for toddlers interview BeDevelI the behavior development screening for toddlers play BeDevelP the Korean version of the childhood autism rating scale KCARS rened from CARS the social communication questionnaire SCQ and the social responsiveness scale SRS The nal diagnosis was based on the best clinical estimate diagnosis according to the DSM ASD criteria by a licensed child psychiatrist using all of the available participant information The participants ages ranged between and months where the average age was months with a standard deviation SD of months Note here that the age means the age at the time when each infant visited the hospital to undergo an initial diagnosis examination There were four males and six females diagnosed with ASD whose average age was months with a SD of The remaining participants consisted of TD children males and females Table displays the collected data distribution while Table shows detailed information of collected data from the infants Table Distribution of age and gender male female Ages MonthNo of Subjects Diagnosed as ASDNo of Subjects Diagnosed as TDNo of Infant Subjects months M F M F months M F M F M F months M F M F Age averageSD Sensors of Table Detailed information on the age gender and initial and denite diagnosis dates of each infant in Table Infant IDAge Months on Initial Diagnosis DateGenderInitial Diagnosis Date Year Month DayDenite Final Diagnosis Date Year Month DayASD TD Male TD Male TD Male TD Male TD Female ASD Male TD Female TD Female TD Male TD Male TD Female ASD Female TD Male TD Female ASD Male TD Female ASD Male ASD Male ASD Female TD Male TD Female TD Male ASD Male ASD Female TD Male TD Male TD Female TD Male ASD Male TD Male TD Male TD Male TD Male TD Female TD Female TD Male TD Male ASD Male TD Male TD As each infants audio data were recorded during the clinical procedure to elicit behaviors from infants with the attendance of one doctor or clinician and one or both parents with the child in the clinical area the audio components consisted of various speeches from the child the clinician and the parents as well as noises from toys or dragging chairs Note here that the recordings were done in one of two typical clinical rooms in SNUBH where the room dimensions were cm cm cm and cm cm cm and the hospital noise level was around dB In order to analyze the vocal characteristics of the infants each audio clip was processed and split into audio segments containing the infants voice not disturbed by music or clattering noises from toys or overlapped by the voices of the clinician or parents Each segment was classied into one of ve categories labeled from to for measuring the data distribution Each label was intended to show di erentiable characteristics relative to the childrens linguistic development for one syllable which is a shortSensors of momentary single vocalization such as ah or ba for two syllables commonly denoted as canonical babbling as a reduplication of clear babbling of two identical or variant syllables such as baba or baga for babbling not containing syllables for rst word such as mother or father and for atypical voice including screaming or crying The distribution of each type of vocalization in seconds is shown in Table The number of vocalizations per category is presented along with a rational value considering the di erence between the ASD and TD groups While the data were unbalanced and very small the distribution of ASD and TD vocalizations show the same tendency as reported in where the ASD group showed a signicantly lower ratio of rst words and an increased ratio of atypical vocalizations revealing developmental delay in linguistic ability Table Amount ratio of each type of vocalization in seconds Vocal Label ASD TD Total For acquiring qualied and e ective feature sets for the vocal data eGeMAPS was employed for voice feature extraction GeMAPS is a popular feature set providing minimalistic speech features generally utilized for automatic voice analysis rather than as a large brute force parameter set As an extended version eGeMAPS contains acoustic features that were fully utilized in this experiment Each recorded set of audio data stored as a kHz stereo le was downsampled and downmixed into a kHz monoaudio le taking into consideration its usability and resolution in melfrequency cepstral coecients MFCCs To extract the speech features for ASD classication each infants utterances were segmented into ms frames with a ms overlap between frames Then di erent features of the eGeMAPS were extracted for each frame with open source speech and music interpretation using the largespace extraction OpenSMILE toolkit and these features were normalized by mean and standard deviation The normalization scaling was acquired and xed by normalizing the factors of the training data set The features were grouped for each ve frames considering the timerelevant characteristics of the speech data PreTrained AE for Acoustic Features To further process and rene the acoustic data a featureextracting AE was introduced An AE is a hierarchical structure that is trained as a regression model for reproducing the input parameters The AE takes inputs and converts them into latent representations and then reconstructs the input parameters from the latent values If we consider an input of AE xRd then the latent representation zRd and the reconstruction of the input yRdare obtained by applying a nonlinear activation function fto the weight sum of zusing a weighting matrix WRddand a bias vector bRd such as zf WTxb yf WTzb where Tis a matrix transpose operator When the latent dimension dd the output from the latent layer is considered to be a compressed meaningful value extracted from the input which is also noted as a bottleneck feature The normalized eGeMAPS features were applied to train the featureextracting AE applying the same data as the input and the target The AE model contained a latent layer with a lowered compacted feature dimension compared to the input layer to achieve the useful bottleneck featureSensors of The model was symmetrically structured centering around the latent layer and the model could be divided into two components the encoder consisting of layers from the input to the latent layers and a decoder consisting of layers from the bottleneck to the output layers The AE structure is depicted in Figure Our AE model consisted of FC layers with the dimensions of and nodes for the input hidden latent hidden and output layers respectively The hidden dimension was selected experimentally and the bottleneck feature dimension was used for comparison with previous research where features were selected considering the statistical dissimilarity of the distributions between the ASD and TD features based on the MannWhitney U test We additionally introduced an auxiliary output as the binary categorical target for ASD and TD which is known as the semisupervised method to train the AE model e ectively The auxiliary output is depicted as Aux in Figure The reconstructed features and auxiliary classication can be written as zifWiizibii where zfWxb and yrecWzb where yrecrefers to the reconstructed eGeMAPS features yauxis the auxiliary classication result fis the activation function and is the softmax activation Sensors x FOR PEER REVIEW of where T is a matrix transpose operator When the latent dimension the output from the latent layer is considered to be a compressed meaningful value extracted from the input which is also noted as a bottleneck feature The normalized eGeMAPS feature s were applied to train the feature extracting AE applying the same data as the input and the target The AE model contained a latent layer with a lowered compacted feature dimension compared to the input layer to achieve the useful bottleneck feature The model was symmetrically structured centering around the latent layer and the model could be divided into two components the encoder consisting of layers from the input to the latent layers and a decoder consisting of layers from the bottleneck to the output layers The AE structure is depicted in Figure Our AE model consisted of FC layers with the dimensions of and nodes for the input hidden latent hidden and output layers respectively The hidden dimension was selected experimentally and the bottleneck feature dimension was used for comparison with previous research where features were selected considering the statistical dissimilarity of the distributions between the ASD and TD features based on the Mann Whitne y U test We additionally introduced an auxiliary output as the binary categorical target for ASD and TD which is known as the semi supervised method to train the AE model effectively The auxiliary output is depicted as Aux in Figure The reconstructed features and auxiliary classification can be written as where and where refers to the reconstructed eGeMAPS features is the auxiliary classification result is the activation function and is the softmax activation Figure Structure of a semi supervised auto encoder AE model eGeMAPS extended version of the Geneva minimalistic acoustic parameter set ASD autism spectrum disorder TD typical development The losses of the reconstruction error for main AE target are measured using the mean absolute error while the auxiliary ASDTD t arget loss is the binary cross entropy and they are added and simultaneously optimized with rational hyper parameters The overall loss equation is Figure Structure of a semisupervised autoencoder AE model eGeMAPS extended version of the Geneva minimalistic acoustic parameter set ASD autism spectrum disorder TD typical development The losses of the reconstruction error for main AE target are measured using the mean absolute error while the auxiliary ASD TD target loss is the binary crossentropy and they are added and simultaneously optimized with rational hyperparameters The overall loss equation is Lrecon NNX i yirecyigt Lauxygtlogyauxtygtlogyaux LtotalLrecon Laux where LreconLaux and Ltotaldenote the reconstruction error auxiliary loss using a binary crossentropy loss function and total loss respectivelySensors of For our stacked AE model a rational value of was selected experimentally considering the proportion of each loss In order to train the AE e ectively both L normalization for weight normalization and batch normalization were adopted After the training was completed we fetched the encoder of the AE as the feature extraction part for the joint optimization model in the training procedures of the deep learning model Establishing and Training the Deep Learning Model for ASD Detection As the eGeMAPS data were set and the AE was trained through semisupervised learning the machine learning models such as SVMs BLSTM and joint optimized BLSTM were constructed Each model had its own input parameter dimensions and the same output targets as ASD and TD classication labels The eGeMAPS feature data were paired with the diagnostic results for the supervised learning of the neural network models For the binary decision ASD was labeled as a positive data point with a label of while TD was labeled as a negative data point We composed four kinds of models with the paired data SVMs with linear kernel the vanilla BLSTM with eGeMAPS features the vanilla BLSTM with eGeMAPS features and the jointly optimized BLSTM layer with the AE The joint optimization model is depicted in Figure As the data set was prepared as the input with ve sequential frames ie the grouped eGeMAPS features in Figure the SVMs received a single frame parameter of dimension which was attened from the original ve input frames For the deep learning models batch normalization rectangular linear unit ReLU activation and dropout were applied for each layer except for the output layer and the adaptive momentum ADAM optimizer was used to train the network The training procedure was controlled by early stopping for minimizing the validation error with epoch patience while saving the best models for improvement of the validation loss by each epoch Because the amount of speech data was relatively small for a deep learning model compared to the disparate eld of audio engineering we grouped the data into ve segments while the test utterances were separated formerly which were selected randomly for of the total data were evenly distributed across each vocalization type and underwent vefold crossvalidation for training then the bestperforming model was chosen Our model was trained with the TensorFlow framework For comparison an SVM model with linear kernel was trained with the same data split as the proposed deep learning model and as well as the vanilla BLSTM suggested in which has single BLSTM with eight cells Sensors x FOR PEER REVIEW of Figure Structure of a joint optimization model of an auto encoder AE and bidirectional long short term memory BLSTM Performance Evaluation The performance of each method was evaluated through five fold cross validation where average ASD utterances and average TD utterances were proportionally distributed over five cases of vocalizations for the gener alized estimation of unconcentrated utterance data The averaged performances of the five validation splits of each model are described in Table The labeled names of the BLSTM were used as the features for training the BLSTM model where eGeMAPS deno tes features of eGeMAPS eGeMAPS denotes features selected by the Mann Whitney U test and AEencoded denotes the joint optimized model In the classification stage one utterance was processed in the frame wise method and the softmax output was c onverted to class indices and and if the average of class indices of the frames was over then the utterance was considered an ASD childs utterance The performances were scored with conventional measures as well as unweighted average recall UA R and weighted average recall WAR chosen in the INTERSPEECH Emotion challenge which considered imbalanced classes In the experiment the SVM model showed very low precision which was extremely biased toward the TD class The BLSTM classifi er with features of eGeMAPS and the AE model showed considerable quality in terms of classifying ASD and TD children while the AE model showed only marginal improvement in correctly classifying children with ASD compared to eGeMAPS The selected features showed degraded quality compared to eGeMAPS obtaining more biased results toward children with TD Table Classification results from the support vector machine SVM BLSTM with or eGeMAPS features selected eGeMAPS features and B LSTM with AE encoded features Models SVM BLSTM eGeMAPS BLSTM eGeMAPS BLSTM AE Encoded Predicted To ASD TD ASD TD ASD TD ASD TD ASD TD Accuracy Precision Recall F score UAR UAR unweighted average recall Figure Structure of a joint optimization model of an autoencoder AE and bidirectional long shortterm memory BLSTM Performance Evaluation The performance of each method was evaluated through vefold cross validation where average ASD utterances and average TD utterances were proportionally distributed over veSensors of cases of vocalizations for the generalized estimation of unconcentrated utterance data The averaged performances of the ve validation splits of each model are described in Table The labeled names of the BLSTM were used as the features for training the BLSTM model where eGeMAPS denotes features of eGeMAPS eGeMAPS denotes features selected by the MannWhitney Utest and AEencoded denotes the joint optimized model In the classication stage one utterance was processed in the framewise method and the softmax output was converted to class indices and and if the average of class indices of the frames was over then the utterance was considered an ASD childs utterance The performances were scored with conventional measures as well as unweighted average recall UAR and weighted average recall WAR chosen in the INTERSPEECH Emotion challenge which considered imbalanced classes In the experiment the SVM model showed very low precision which was extremely biased toward the TD class The BLSTM classier with features of eGeMAPS and the AE model showed considerable quality in terms of classifying ASD and TD children while the AE model showed only marginal improvement in correctly classifying children with ASD compared to eGeMAPS The selected features showed degraded quality compared to eGeMAPS obtaining more biased results toward children with TD Table Classication results from the support vector machine SVM BLSTM with or eGeMAPS features selected eGeMAPS features and BLSTM with AEencoded features Models SVMBLSTM eGeMAPSBLSTM eGeMAPSBLSTM AEEncoded Predicted To ASD TD ASD TD ASD TD ASD TD ASD TD Accuracy Precision Recall F score UAR UAR unweighted average recall Discussion The vanilla BLSTM model presented in conducted discrimination on wellclassied subjects with monthold children and sorted features from eGeMAPS that had a distinctive distribution between ASD and TD selected by the MannWhitney Utest using the threefold crossvalidation method However because the di erence in the data distribution failed to achieve the same eGeMAPS feature selection between the test and classication results with the specied feature set presented herein the application of an identical model structure and the adoption of the same feature domain will allow both approaches to be indirectly comparable These results can be interpreted by the data distributions and we performed tstochastic neighbor embedding tSNE analysis on the training data set which can nonlinearly squeeze the data dimension based on a machine learning algorithm Figure shows each data distribution as a twodimensional scatter plot In the gure the eGeMAPS features from eGeMAPS and eGeMAPS showed almost identical distribution except for the amount of ASD outliers which implies that the ASD and TD features in the eGeMAPS features show similar distributions in this experiment As shown in eGeMAPS includes temporal features that are relevant to vocalizations and utterances thus these features might cause confusion regarding the discrimination between ASD and TD The AEencoded features however showed a redistributed feature map with a more characteristic distribution compared to the eGeMAPS features This is because the AEencoded features were compressed into a bottleneck feature which was derived by weighting the matrix paying attention to the signicant parametersSensors of while reducing the inuence from the ambiguous parameters While the joint optimization model achieved only marginally improved results compared to eGeMAPS the distribution of the feature map would be more noticeable in improved feature extraction models as well as more di erentiable in complex models although BLSTM with eight cells was employed for a comparison with conventional research in this experiment Sensors x FOR PEER REVIEW of Discussion The vanilla BLSTM model presented in conducted discrimination on well classified subjects with month old children and sorted features from eGeMAPS that had a distinctive distribution between ASD and TD selected by the Mann Whitney U test using t he three fold cross validation method However because the difference in the data distribution failed to achieve the same eGeMAPS feature selection between the test and classification results with the specified feature set presented herein the applicatio n of an identical model structure and the adoption of the same feature domain will allow both approaches to be indirectly comparable These results can be interpreted by the data distributions and we performed t stochastic neighbor embedding t SNE analy sis on the training data set which can nonlinearly squeeze the data dimension based on a machine learning algorithm Figure shows each data distribution as a twodimensional scatter plot In the figure the eGeMAPS features from eGeMAPS and eGeM APS showed almost identical distribution except for the amount of ASD outliers which implies that the ASD and TD features in the eGeMAPS features show similar distributions in this experiment As shown in eGeMAPS includes temporal features that are relevant to vocalizations and utterances thus these features might cause confusion regarding the discrimination between ASD and TD The AEencoded features however showed a redistributed feature map with a more characteristic distribution compared to the eGeMAPS features This is because the AE encoded features were compressed into a bottleneck feature which was derived by weighting the matrix paying attention to the significant parameters while reducing the influence from the ambiguous parameters While the joint optimization model achieved only marginally improved results compared to eGeMAPS the distribution of the feature map would be more noticeable in improved feature extraction models as well as more differentiable in complex models alt hough BLSTM with eight cells was employed for a comparison with conventional research in this experiment While the overall performance scores were comparably low for general classification problems on account of the subjectivity and complexity of problems and the limitation in terms of the shortage of data the results of the jointly optimized model imply the possibility of deep learning based feature extraction for the improvement of automated ASDTD diagnosis under restricted circumstances a b c Figure Two dimensional scatter plot for a eGeMAPS b eGeMAPS and c the AE processed by t stochastic neighbor embedding t SNE Conclusion s In this paper we conducted experiments for discovering the possibility of auto encoder based feature extraction and a joint optimization method for the automated detection of atypicality in voices of children with ASD during early developmental stages Un der the condition of an insufficient and dispersed data set the clas sification results were relatively poor in comparison to the general classification tasks based on deep learning Although our investigation used a limited number of subjects and an unbal anced data set the suggested auto encoder based feature extraction and joint optimization method revealed the possibility of feature dimension and a slight improvement in model based diagnosis under such uncertain circumstances Figure Twodimensional scatter plot for a eGeMAPS b eGeMAPS and c the AE processed by tstochastic neighbor embedding tSNE While the overall performance scores were comparably low for general classication problems on account of the subjectivity and complexity of problems and the limitation in terms of the shortage of data the results of the jointly optimized model imply the possibility of deeplearningbased feature extraction for the improvement of automated ASD TD diagnosis under restricted circumstances Conclusions In this paper we conducted experiments for discovering the possibility of autoencoderbased feature extraction and a joint optimization method for the automated detection of atypicality in voices of children with ASD during early developmental stages Under the condition of an insu cient and dispersed data set the classication results were relatively poor in comparison to the general classication tasks based on deep learning Although our investigation used a limited number of subjects and an unbalanced data set the suggested autoencoderbased feature extraction and joint optimization method revealed the possibility of feature dimension and a slight improvement in modelbased diagnosis under such uncertain circumstances In future work we will focus on increasing the reliability of the proposed method by addition of a number of infants speech data renement of the acoustic features an autoencoder for feature extraction and better deeper and uptodate model structures This research can also be extended to children with the age of or who can speak several sentences In this case we will investigate the linguistic features as well as acoustic features such as we have done in this paper In addition to ASD detection this research can be applied to the detection of infants with development delays Author Contributions All authors discussed the contents of the manuscript HKK contributed to the research idea and the framework of this study GB and HJY provided the database and helped with the discussion JHL performed the experiments GWL contributed to the data collection and preprocessing All authors have read and agreed to the published version of the manuscript Funding This work was supported by the Institute of Information communications Technology Planning evaluation IITP grant funded by the Korea government MSIT No Development of AI Technology for Early Screening of Infant Child Autism Spectrum Disorders based on Cognition of the Psychological Behavior and Response Conicts of Interest The authors declare no conict of interestSensors of References National Institute of Mental Health Autism Spectrum Disorder Available online govhealth topics autismspectrumdisordersasd indexshtml accessed on October American Psychiatric Association Diagnostic and Statistical Manual of Mental Disorders DSM American Psychiatric Publishing Washington DC USA Centers for Disease Control and Prevention CDC Data Statistics on Autism Spectrum Disorder Available online ncbddd autism datahtml accessed on October Fenske EC Zalenski S Krantz P J McClannahan LE Age at intervention and treatment outcome for autistic children in a comprehensive intervention program Anal Interv Devel Disabil CrossRef Falkmer T Anderson K Falkmer M Horlin C Diagnostic procedures in autism spectrum disorders A systematic literature review Eur Child Adolesc Psychiatry CrossRef PubMed Bailey A Le Couteur A Gottesman I Bolton P Simono E Yuzda E Rutter M Autism as a strongly genetic disorder Evidence from a British twin study Physiol Med CrossRef PubMed Du y FH Als H A stable pattern of EEG spectral coherence distinguishes children with autism from neurotypical controlsA large case control study BMC Med CrossRef PubMed Chaspari T Lee CC Narayanan SS Interplay between verbal response latency and physiology of children with autism during ECA interactions In Proceedings of the Annual Conference of the International Speech Communication Association Interspeech Portland OR USA September pp BaronCohen S Social and pragmatic decits in autism Cognitive or a ective J Autism Dev Disord CrossRef PubMed Bonneh YS Levanon Y DeanPardo O Lossos L Adini Y Abnormal speech spectrum and increased pitch variability in young autistic children Front Hum Neurosci CrossRef PubMed Chericoni N de Brito Wanderley D Costanzo V DinizGonalves A Gille ML Parlato E Cohen D Apicella F Calderoni S Muratori F Prelinguistic vocal trajectories at months of age as early markers of autism Front Psychol CrossRef PubMed Alom MZ Taha TM Yakopcic C Westberg S Sidike P Nasrin MS Hasan M van Essen BC Awwal AAS Asari V K A stateoftheart survey on deep learning theory and architectures Electronics CrossRef Song DY Kim SY Bong G Kim JM Yoo HJ The use of articial intelligence in screening and diagnosis of autism spectrum disorder A literature review J Korean Acad Child Adolesc Psychiatry CrossRef PubMed Santos JF Brosh N Falk TH Zwaigenbaum L Bryson SE Roberts W Smith IM Szatmari P Brian JA Very early detection of autism spectrum disorders based on acoustic analysis of preverbal vocalizations of month old toddlers In Proceedings of the IEEE International Conference on Acoustics Speech and Signal Processing ICASSP Vancouver BC Canada May pp Li M Tang D Zeng J Zhou T Zhu H Chen B Zou X An automated assessment framework for atypical prosody and stereotyped idiosyncratic phrases related to autism spectrum disorder Comput Speech Lang CrossRef Eyben F Scherer KR Schuller BW Sundberg J Andr E Busso C Devillers LY Epps J Laukka P Narayanan SS et al The Geneva minimalistic acoustic parameter set GeMAPS for voice research and a ective computing IEEE Trans A ect Comput CrossRef Pokorny FB Schuller BW Marschik P B Brueckner R Nystrm P Cummins N Blte S Einspieler C FalckYtter T Earlier identication of children with autism spectrum disorder An automatic vocalisationbased approach In Proceedings of the Annual Conference of the International Speech Communication Association Interspeech Stockholm Sweden August pp Xing C Ma L Yang X Stacked denoise autoencoder based feature extraction and classication for hyperspectral images J Sens CrossRef Bong G Kim J Hong Y Yoon N Sunwoo H Jang J Oh M Lee K Jung S Yoo H The feasibility and validity of autism spectrum disorder screening instrument Behavior development screening for toddlers BeDevelA pilot study Autism Res CrossRef PubMedSensors of Center for Autism Research Social Communication Questionnaire SCQ Available online www carautismroadmaporg socialcommunicationquestionnairescq print pdf accessed on October Center for Autism Research Childhood Autism Rating Scale nd Edition CARS Available online childhoodautismratingscale print pdf accessed on October Center for Autism Research Social Responsiveness Scale nd Edition SRS Available online socialresponsivenessscale print pdf accessed on October Eyben F Wllmer M Schuller B OpenSMILEThe Munich versatile and fast opensource audio feature extractor In Proceedings of the th ACM International Conference on Multimedia Firenze Italy October pp Masci J Meier U Cire san D Schmidhuber J Stacked Convolutional AutoEncoders for Hierarchical Feature Extraction In Articial Neural Networks and MachineICANN Honkela T Duch W Girolami M Kaski S Eds Springer Berlin Heidelberg Germany pp Sainath T Kingsbury B Ramabhadran B Autoencoder bottleneck features using deep belief networks In Proceedings of the IEEE International Conference on Acoustics Speech and Signal Processing ICASSP Kyoto Japan March Nachar N The MannWhitney U A test for assessing whether two independent samples come from the same distribution Tutor Quant Methods Psychol CrossRef Le L Patterson A White M Supervised autoencoders Improving generalization performance with unsupervised regularizers In Advances in Neural Information Processing Systems Bengio S Wallach H Larochelle H Grauman K CesaBianchi N Garnett R Eds Curran Associates Inc New York NY USA pp van Laarhoven T L regularization versus batch and weight normalization arXiv arXiv Io e S Szegedy C Batch normalization Accelerating deep network training by reducing internal covariate shift In Proceedings of the International Conference on Machine Learning Lille France July pp Nair V Hinton GE Rectied linear units improve restricted Boltzmann machines In Proceedings of the th International Conference on Machine Learning Haifa Israel June pp Srivastava N Hinton G Krizhevsky A Sutskever I Salakhutdinov R Dropout A simple way to prevent neural networks from overtting J Mach Learn Res Kingma DP Ba JL ADAM A method for stochastic optimization In Proceedings of the rd International Conference on Learning Representations San Diego CA USA May pp Abadi M Barham P Chen J Chen Z Davis A Dean J Devin M Ghemawat S Irving G Isard M et al TensorFlow A system for largescale machine learning In Proceedings of the th USENIX Symposium on Operating Systems Design and Implementation Savannah GA USA November pp Schuller B Steidl S Batliner A The Interspeech emotion challenge In Proceedings of the Annual Conference of the International Speech Communication Association Interspeech Brighton UK September pp van der Maaten L Hinton G Visualizing data using tSNE J Mach Learn Res Publishers Note MDPI stays neutral with regard to jurisdictional claims in published maps and institutional aliations by the authors Licensee MDPI Basel Switzerland This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution CC BY license creativecommonsorg licenses by