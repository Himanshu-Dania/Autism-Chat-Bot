RobotAssistedAutismSpectrumDisorderDiagnosticBased onArticialReasoning AndresARam rezDuqueAnselmoFrizeraNetoTeodianoFreireBastos ReceivedAprilAcceptedDecember SpringerNatureBV Abstract Autism spectrum disorder ASD is a neurodevelopmental disorder that affects people from birth whose symptoms are found in the early developmental period The ASD diagnosis is usually performed through several sessions of behavioral observation exhaustive screening and manual coding behavior The early detection of ASD signs in naturalistic behavioral observation may be improved through ChildRobot Interaction CRI and technologicalbased tools for automated behavior assessment Robotassisted tools using CRI theories have been of interest in intervention for children with Autism Spectrum Disorder CwASD elucidating faster and more significant gains from the diagnosis and therapeutic intervention when compared to classical methods Additionally using computer vision to analyze childs behaviors and automated video coding to summarize the responses would help clinicians to reduce the delay of ASD diagnosis In this article a CRI to enhance the traditional tools for ASD diagnosis is proposed The system relies on computer vision and an unstructured and scalable network of RGBD sensors built upon Robot Operating System ROS and machine learning algorithms for automated face analysis Also a proof of concept is presented with participation of three typically developing TD children and three children in risk of suffering from ASD Keywords ChildRobot interaction Autism spectrum disorder Convolutional neural network Robot reasoning model Statistical shape modeling Introduction Research in ChildRobot Interaction CRI aims to provide the necessary conditions for the interaction between a child and a robotic device taking into account some fundamental features such as childs neurophysical and physical condition and the childs mental health That is how RobotAssisted Therapies RAT using CRI theories have been of interest as an intervention for CwASD elucidating faster and more significant gains from the therapeutic intervention when compared to traditional therapies ASD is a neurodevelopmental disorder that affects people from birth and its symptoms are found in the early envelopebackAndr es A Ram rezDuque Universidade Federal do Espir to Santo Av Fernando Ferrari Vitoria Brazildevelopmental period Individuals suffering from ASD exhibit persistent deficits in social communication social interaction and repetitive patterns of behavior interests or activities Some of the ASD signs may be observed before the age of months although a reliable diagnosis can only be performed at months of age according to or months according to The use of computer vision to analyze the childs behaviors and automated video coding to summarize the interventions can help the clinicians to reduce the delay of ASD diagnosis providing the CwASD with access to early therapeutic interventions In addition CRIbased intervention can transform traditional diagnosis methods through a robotic device to systematically elicit childs behaviors that exhibit ASD signs Some of the first systems developed to assist ASD therapists and make diagnosis based on robotic devices have primarily been open loop and remotely operated sys tems However these approaches are unable to perform autonomous feedback to enhance the interaction JournalofIntelligentRoboticSystems Publishedonline March Content courtesy of Springer Nature terms of use apply Rights reservedNevertheless different systems are able to modify the behavior of the robot according to environmental interac tions and the childs response using a closedloop and artificial cognition approaches These systems have been hypothesized to offer technological mechanisms for supporting more flexible and potentially more naturalistic interaction In fact literature reports that automatic robots social behaviors modulation according to specifics scenarios has a strong effect on childs social behavior However despite the increase of positive evidence this technology has rarely been applied to specific ASD diagnosis This work aims to present a robotassisted framework using an artificial reasoning module to assist clinicians with the ASD diagnostic process The framework is composed of a responsive robotic platform a flexible and scalable vision sensor network and an automated face analysis algorithm based on machine learning models In this research we take advantage of some neural models available as open sources projects to build a completely new pipeline algorithm for global recognition and tracking of childs face among many faces present in a typical unstructured clinical intervention in order to estimate the childs visual focus of attention along the time The proposed system can be used in different behavioral analysis scenarios typical of an ASD diagnostic process In order to illustrate the feasibility of the proposed system in this paper an experimental trial to assess joint attention behavior is presented employing an inclinic setup unstructured environment The main contributions of this paper are i the development of a new artificial reasoning module upon a flexible and scalable ROSbased vision system using stateoftheart machine learning neural models ii the proposal and implementation of a supervised CRI child robot interaction based on an open source social robotic platform to enhance the traditional tools for ASD diagnosis using an inclinic setup protocol For the best of our knowledge there are no open source projects available for face analysis based on a multicamera approach using ROS with the characteristics described in our research RelatedWork Recent researches have shown the acceptance and efficiency of technologies used as auxiliary tools for therapy and teaching of individuals with ASD Such technologies may also be useful for people surrounding ASD individuals therapists caregivers family members For example the use of artificial vision systems to measure and analyze the childs behavior can lead to alternative screening and monitoring tools that help the clinicians to get feedback from the effectiveness of the intervention Additionally social robots have great potential for aid in the diagnosis and therapy of children with ASD A higher degree of control prediction and simplicity may be achieved in interactions with robots impacting directly on frustration and reducing the anxiety of these individuals Respect to the use of computer vision techniques previous studies already analyzed childs behaviors such as visual attention eye gaze eye contact smile events and visual exploration using cameras and eye trackers and RGBd cameras These studies have shown the potential of vision systems in improving the behavioral coding in ASD therapies However these studies did not implement techniques of CRI to enhance the intervention On the other hand studies about how CwASD respond to a robot mediator compared to human mediator have been reported such as intervention scenarios with imitation games telling stories and free play tasks These works used features such as proxemics body gestures visual contact and eye gaze as behavioral descriptors whereas the behavior analysis was estimated using manual video coding Researchers of Vanderbilt University published a series of research showing an experimental protocol to assess joint attention JA tasks defined as the capacity for coordinated orientation of two people toward an object or event The protocol consisted of directing the attention of the child towards objects located in the room through adaptive prompts Bekele et al inferred the participants eye gaze by the head pose which was calculated in realtime by an IR camera array In their last works Zheng et al and Warren et al used a commercial eye tracker to estimated the childrens eye gaze around the robot and manual behavioral coding for global evaluation However eye tracker devices require precalibration and may limit the movement of the individual The results of these works showed that the robot attracted childrens attention and that CwASD reached all JA task Nevertheless developing JA tasks is more difficult with a robot than with humans Anzalone et al developed a CRI scenario using the NAO robot to perform JA tasks in which the authors used an RGBD camera to estimate only body and head movements The results showed that JA performance of children with ASD was similar to the performance of TD children when interacting with the human mediator however with a robot mediator the children with ASD presented a lower performance than the TD children ie the children with ASD needed more social cues to finalize the task Chevalier et al analyzed in their study some features such as proprioceptive and visual integration in CwASD using an RGBD sensor to record the interventions sessions and manual behavior coding to analyzed the participants performance In none of the previous works a closedloop subsystem wasJIntellRobotSyst Content courtesy of Springer Nature terms of use apply Rights reservedimplemented to provide some level of artificial cognition to enable automated robot behavior In contrast with the aforementioned researches other works implemented automated face analysis and artificial cognition through robotmediator and computer vision which analyzed childs engagement emotions recognition capability and childs intentions In these works two different strategies were implemented where the most common is based on mono camera approach using an external RGB or RGBd sensor or using onboard RGB cameras mounted on the roboticplatform Other strategies are based on a highly structured environment composed of an external camera plus an onboard camera o ra network of vision sensors attached to a small table These strategies based on multicamera methods improve the systems performance but remain constrained in relation to desired features such as flexibility scalability and modularity Thus despite the potential that these techniques have shown achieving automated childs behavior analysis in a naturalistic way into unstructured clinicalsetups with robots that interact accordingly remains a challenge in CRI SystemArchitectureOverview The ROS system used in this work is a flexible and scalable open framework for writing modular robot centered systems Similar to a computing operating system ROS manages the interface between robot hardware and software modules and provides common device drivers data structures and toolbased packages such as visualization and debugging tools In addition ROS uses an interface definition language IDL to describe the messages sent between process or nodes this feature facilitates the multi language C Python and Lisp development The overall system developed here was built using a node graph architecture taking advantages of the principal ROS design criteria As with ROS our system consists of a number of nodes to local video processing together a robots behavior estimation distributed around a number of different hosts and connected at runtime in a peerto peer topology The internode connection is implemented as a handshaking and occurs in XMLRPC protocol along with a websocket communication for robots web based node ONO node see Fig The node structure is flexible scalable and can be dynamically modified ie each node can be started and left running along an experimental session or resumed and connected to each other at runtime In addition from a general perspective any robotic platform with websocket communication can be integrated The developed system is composed of two interconnected modules as shown in Fig an artificialreasoning module and a CRIchannel module The module architectures are detailed in the following subsections ArchitectureofReasoningModule In this module a distributed architecture for local video processing is implemented The data of each RGBD sensor in the multicamera system are processed for two nodes in which the first is a driver level node and the second is a processing node The drivernode transforms the streaming data of the RGBD sensor into the ROS messages format The driver addresses the data through a specialized transport provided by plugings to publishes images in a compressed representations while the receptor node only sees sensor msgsImage messages The data processing node executes the face analysis algorithm This node uses aimage transport subscriber and a ROS packages called CvBridge to turn the data into a image format supported for the typical computer vision algorithms Later the same node publishes the head pose and eye gaze direction by means of a ROS navigation message defined as nav msgsOdometry An additional node hosted in the most powerful workstation carries out a data fusion of all navigation messages that were generated in the local processing stage In addition to the fusion this node computes the visual focus of attention VFOA and publishes this as a std msgsHeader in which the time stamp and the target name of the VFOA estimation are registered ArchitectureofCRIChannel The system proposed here has two bidirectional communi cation channels a robotdevice and a webbased applica tion to interact with both the child and the therapist The robot device can interact with the CwASD executing differ ent physical actions such as facial expression upper limb poses and verbal communication Thus according to the childs performance the reasoning module can modify the robots behavior through automatic gaze shifting chang ing the facial expression and providing sound rewards The clientside application was developed to allow the therapist to control and register all step of the intervention proto col This interface was also used to supervise and control the robots behavior and to offer feedback to the therapist about the childs performance along the intervention This App has two channels of communication for interacting with the reasoning module The first connection uses a web socket protocol and a RosBridge suite package to support the interpretation of ROS messages as well as JSONbased commands in ROS The second one uses a ROS module Tools for using the Kinect One Kinect V in ROS comcodeiaiiai kinect JIntellRobotSyst Content courtesy of Springer Nature terms of use apply Rights reservedFig Node graph architecture of the proposed ROSbased system The system is composed of two interconnected modules an artificial reasoning module and a CRIchannel module The ONO web server has two way of bidirectional communication a websocket and a standard ROS Subscriber developed in the serverside application to directly run a ROS node and communicate with standard ROS publishers and subscribers TheRoboticPlatformONO The CRI is implemented through the open source platform for social robotics OPSOROwhich is a promising and straightforward system developed for face to face communication composed of a lowcost modular robot called ONO see Fig and webbased applications Some of the most important requirements and characteristics that make ONO interesting for this CRI strategy are explained in the following sections AppearanceandIdentity The robot is covered in foam and also fabric to have a more inviting and huggable appearance to the children The robot has an oversized head to make its facial expressions more prominent and to highlight the importance for communica tion and emotional interaction As a consequence of its size and pose children can interact with the robot at eye height when the robot is placed on a table The robot ONO has not a predefined identity as the only element previously conceived is the name Unlike other robots that have welldefined identities such as Probo or Kaspar in this work the ONOs identity is built with the participation of the child through a cocreation process For this reason a neutral appearance is initially used In the Open Source Platform for Social Robotics OPSORO opsorocom intervention the therapist can provide the child with clothes and accessories to define the identity of ONO MechanicsPlatform As the initial design of ONO is composed only of the actuated face in this work it was needed to provide the ONO with some body language For this purpose motorized arms were designed and implemented The new design of ONO has a fully face and two arms actuated giving a total of Degrees of Freedom DOF The ONO is able to perform facial expressions and nonverbal cues such as waving shake hands and pointing towards objects moving its arms DOF x eyes DOF x eyelids DOF x eyebrows DOF x and mouth DOF The robot has also a sound module that allows explicit positive feedback as well as reinforcement learning through playing words conversations and other sounds SocialExpressiveness In order to improve social interaction with a child the ONO is able to exhibit different facial expressions The ONOs expressiveness is based on the Facial Action Coding System FACS developed in Each DOF that composes the ONOs face is linked with a set of Action Units AU defined by the FACT and each facial expression is determined for specific AU values The facial expressions are represented as a D vector fev a in the emotion circumplex model defined by valence and arousal In this context the basic facial expressions are specified on a unit circle where the neutral expression corresponds to the origin of the space fe The relation between the DOF position and the AU values is resolved through a lookup table algorithm using a predefined configuration file JIntellRobotSyst Content courtesy of Springer Nature terms of use apply Rights reservedFig ONO robot developed through the open source platform for social robotics OPSORO AdaptabilityandReproducibility The application of the DoItYourself DIY concept is the principal feature of ONOs design which facilitates its dissemination and use in research areas other than engineering as health care These characteristics allow ONO building for any person without specialized engineering knowledge Additionally it is possible to replicate ONO without the need for highend components or manufacturing machines The electronic system is based on a Raspberry Pi singleboard computer combined with a custom OPSORO module with circuitry to control up to servos drive speakers and touch sensors Any sensor or actuator compatible with the embedded communication protocols UART IC SPI implemented on the Raspberry Pi can be used by this platform ControlandAutonomy With the information delivered for the automated reasoning module it was possible to automate the ONOs behavior and then the robot can infer and interpret the childrens intentions to react most accurately to the action performed by them thus enabling a more efficient and dynamic interaction with ONO In this work the automated ONOs behavior is partially implemented ie the framework can modify some physical actions of ONO using the feedback information about the childs behavior The actions suitable to be modified are gaze shift toward the child in specifics events changing from neutral to positive facial expression when the child looks toward the target and providing sound rewards Also an Aliveness Behavior Module ABM is implemented to improve the CRI which consist of blinking the robots eyes and changing its arms amongsome predefined poses Also the robot can be manually operated through a remote controller hosted in the client side application ReasoningModuleMachineLearning MethodsforChildsFaceAnalysis The automated childs face analysis consists of monitoring nonverbal cues such as head and body movements head pose eye gaze visual contact and visual focus of attention In this work a pipeline algorithm is implemented using machine learning neural models for face analysis The chosen methods were developed using stateofart trained neural models available by Dlib and OpenFace Some modification such as turn the neural model an attribute of the ROS node class and evaluate this in each topic callback were needed to run the neural models into a common ROS node The algorithm proposed for childs face analysis involves face detection recognition segmentation and tracking landmarks detection and tracking head pose eye gaze and visual focus of attention VFOA estimation In addition the architecture proposed here also implement new methods for asynchronous matching and fusion of all local data visual focus of attention estimation based on Hidden Markov Model HMM and direct connection with the CRIchannel to influence the robots behaviors A scheme of the pipeline algorithm is shown in Fig Dlib C Library A Open Source Facial Behavior Analysis TadasBaltrusaitisOpenFace JIntellRobotSyst Content courtesy of Springer Nature terms of use apply Rights reservedFig Pipeline algorithm of the automated childs face analysis ChildsFaceDetectionandRecognition The inclinic setup requires differentiate the childs face from other faces detected and found in the scene For this reason a face recognition process was also implemented in this work First the face detection is executed to initialize the face recognition process and subsequently initialize the landmarks detection In this work both detection and recognition are implemented using deep learning models which are described in this section In the detection process a Convolutional Neural Network CNN based face detector with a MaxMargin Object Detection MMOD as loss layer is used The CNN consist first of a block composed of three downsampling layers which apply convolution with a x filter size and stride to reduce the size of the image up to eight times its original size and generate a feature map with dimensions Later the result are processed for one more block composed of four convolutional layers to get the final output of the network The three first layers of the last block have filter size and x stride but the last layer has only channel and a filter size The values in the last channel are large when the network thinks it has found a face at a particular location All convolutional block above are implemented with two additional layers among convolutional layers pointwise linear transformation and Rectified Linear Units RELU to apply the nonsaturating activation function f xmaxx The training dataset used to create the model is composed of faces and is available at Dlibs homepage The face recognition algorithm used in this work is inspired on the deep residual model from The face detection datasettar gzresidual network ResNet model developed by He et al reformulates the convolutional layers to learn a residual functions Fx Hx xwith reference to the layer inputs x instead of learning unreferenced functions In the practical implementation the previous formulation means inserting shortcut connections which turn the network into its counterpart residual version The CNN model then transforms each face detected to a D vector space in which images from the same person will be close to each other but faces from different people will be far apart Finally the faces are classified as childs face caregivers face and therapists face Both detection and recognition CNN model were implemented and trained from and released in Dlib FaceAnalysisLandmarksHeadPoseandEye Gaze This work uses the technique for landmarks detection head pose and eye gaze estimation developed by Baltru saitis et al named Conditional Local Neural Fields CLNF This technique is an extension of the Constrained Local Model CLM algorithm using specialized local detectors or patch experts CNLF model consists of a statistical shape model which its learned from data examples and is parametrized for mcomponents of linear deformation to control the possible shape variations of the nonrigid objects Approaches based on CLM a n dC L N F model the object appearance in a local fashion ie each feature point has its own appearance model to describe the amount of misalignment CLNFbased landmark detection consists of three main parts the shape model the local detectors or patch experts and the fitting algorithm which are detailed belowJIntellRobotSyst Content courtesy of Springer Nature terms of use apply Rights reservedShapeModel The CLNF technique uses a linear model to describe non rigid deformations called Point Distribution Model PDM The PDM is used to estimate the likelihood of the shapes being in a specific class given a set of feature points This is important for model fitting and shape recognition The shape of a face that has nlandmark points can be described as XXXX nYYY nZZZ n and the class that describes a valid instance of a face using PDM can be represented as XXPhiq where Xis the mean shape of the face Phidescribed the principal deformation modes of the shape and q represent the nonrigid deformation parameters Both X andPhiare learned automatically from labeled data using Principal Component Analysis PCA The probability density distribution of the instances into the shape class is expressed as a zero mean Gaussian with Covariance matrix Lambdamevaluated at q pqNqLambdamLambdaexpbraceleftbigg qTLambdaqbracerightbigg Once the model is defined it is necessary to place the D PDM in an image space The following equation is used to transform between D space to image space using weak perspective projection xisRDXiPhiiqt where Xi xiyiziTis the mean value of the ith landmark The instance of the face in an image is therefore controlled using the parameter vector pswtq where qrepresents the local nonrigid deformation sis a scaling term wis the rotation term that controls the matrix RDa n dtis the translation term The global parameters are used to estimate the head pose in reference to the camera space using orthographic camera projection and solving the PerspectivenPoint PnP problem respect to the detected landmarks The PDM used in was trained on two public datasets This result in a model with nonrigid Principal modes and rigid shape parameters PatchExperts The patch experts scheme is the main novelty implemented in the CLNF model The new Local Neural Field LNFpatch expert takes advantage of the non linear relationship between pixel values and the patch response maps The LNF captures two kinds of spatial characteristics between pixels such as similarity and sparsity LNF patch expert can be interpreted as a three layer perceptron with a sigmoid activation function followed by a weighted sum of the hidden layers It is also similar to the first layer of a Convolutional Neural Network The new LNF patch expert is able to learn from multiple illuminations and retain accuracy This becomes important when creating landmark detectors and trackers that are expected to work in unseen environments and on unseen people The learning and inference process is developed using a gradientbased optimization method to help in finding locally optimal model parameters faster and more accu rately In the CLNF model implemented in set in total of LNF patch experts were trained for seven views and four scales The framework uses patch experts specifically trained to recognize the eyelids iris and the pupil in order to estimate the eye gaze FittingAlgorithm For each new image or video frame the fitting algorithm of CLNFbased landmark detection process attempts to find the value of the local and global deformable model parameters pthat minimizes the following function EpRpnsummationdisplay iDixiI whereRis a weight to penalize unlikely shapes which depends on the shape model and Drepresents the misalignment of the ithlandmark in the image Iw h i c h is function of both the parameters pand the patch experts Under the probabilistic point of view the solution of i s equivalent to maximize the a posteriori probability MAP of the deformable model parameters p pparenleftbig plin iIparenrightbig ppnproductdisplay iplixiI where liis a discrete random variable indicating whether the ithlandmark is aligned or misaligned pp is the prior probability of the deformable parameters p andplixiIis the probability of a landmark being aligned at a particular pixel location xi which is quantified from the response maps created by patch Therefore the last term in represents the joint probability of the patch expert response maps The MAP problem is solved using a optimization strategy designed specifically for CLNF fitting called nonuniformJIntellRobotSyst Content courtesy of Springer Nature terms of use apply Rights reservedregularized landmark mean shift NURLMS which uses two step process The first step evaluates each of the patch experts around the current landmark using a Gaussian Kernel Density Estimator KDE The second step iteratively updates the model parameters to maximize The NURLMS uses expectation maximization algo rithm where the Estep involves evaluating the posterior probability over the candidates and the Mstep finds the parameter updated through the mean shift vector vT h e mean shift vector points in the direction where the feature point should go but the motion is restricted by the statisti cal shape model and the Rp This interpretation leads to the new update function argmin DeltapbraceleftBig bardblJDelta pvbardbl WrbardblpDeltapbardbl LambdabracerightBig where ris a regularization term Jis the Jacobian which describe how the landmarks location are changing based on the infinitesimal changes of the parameters pLambda diag ma n d Wallows for weighting of meanshift vectors Nonlinear least squares leads to the following update rule DeltapparenleftBig JTWJrLambdaparenrightBigparenleftBig rLambdapJTWvparenrightBig To construct W the performance of patch experts on training data is used DataFusion The fusion of the local results for the head pose estimation is done applying a consensus over the rotation algorithm This algorithm consists of calculating the weighted average pose between each camera estimation and its immediate sensors estimation neighbors using the axis angle representation The local pose is penalized by two weights the alignment confidence of landmarks detection procedure and the Mahalanobis distances between the head pose and a neutral pose FieldofViewFoVandVisualfocusofAttention VFOA The VFOA estimation model is implemented as a dynamic Bayesian network through a Hidden Markov Model HMM The model assumes a specific set of childs attention attractors or targets F The estimation process decodes the sequence of childs head poses Ht Hyaw tHpitch tRin terms of VFOA states FtF at time t The probability distribution of the head poses in reference to a given VFOA target is represented by a Gaussian distribution whereas the transitions amongthese targets are represented by the transition matrix AT h e HMM equations can then be written as follows PH tFtfh tNHth tf Sigma Hf pFtfFtfAff The Gaussian covariances is defined manually to reflect target sizes and head pose estimation variability Moreover the Gaussian means corresponding to each specific target h tis calculated through a gaze model that sets this parameter as a fixed linear combination of the target direction and the head reference direction h tf tf R t where denotes the component wise product yawpitchare adjustable constants that describe the fraction of the gaze shift that corresponds to the childs head rotation tRKis the directions of the given K targets and RtRrepresents the reference direction which is the average head pose over a time window WR The above assumption describes the body orientation behavior of any child who tends to orient himselfherself towards the set of gaze targets to make more comfortable to rotate hisher head towards different targets Rt WRtsummationdisplay itWRHi Finally for the estimation of the VFOA sequence a classic Viterbi algorithm of HMM is implemented CaseStudy For the case study the vision system is composed of three Kinect V sensors Each sensor is connected to a workstation equipped with a processor of Intel Core i family and a GeForce GTX GPU board two workstation with GTX board and one workstation with GTX board All workstation are connected through a local area network synchronized using the NTP protocolThe sensors were intrinsically and extrinsically calibrated through a conventional calibration process using a standard black white chessboard InclinicSetup A multidisciplinary team of psychologists doctors and engineers developed a case study using a psychology room equipped with a unidirectional mirror to perform behavioral Network Time Protocol Homepage Tools for using the Kinect One Kinect V in ROS comcodeiaiiai kinect JIntellRobotSyst Content courtesy of Springer Nature terms of use apply Rights reservedFig Representation of the interventions room of inclinic setup observation appropriately The room was prepared with a table and three chairs one for the child another for the caregiver and a third one for the therapist The robot was placed on the table and the following toys a helicopter a truck and a train were attached to rooms walls The RGBD sensors were located close to the walls and no additional camera was placed on the robot or the table so as not to attract the childs attention A representation of the interventions room of inclinic setup is shown in Fig InterventionProtocol In this work a technologybased system was used as a tool in various stages of the ASD diagnostic process The framework can be implemented to extract differentbehavioral features to be assessed eg eye contact stereotyped movements of the head concentration and excessive interest in objects or events However for the scope of this research a specific clinical setup intervention to assess Joint Attention JA behaviors is presented The intervention aims to evaluate the capacity of JA which can be divided into three classes initiation of joint attention IJA responding to joint attention bids RJA and initiation of request behavior IRB The therapist guides the intervention all the time and leverages the robot device as an alternative channel of communication with the child for the above both the specialist and the robot remained in the room during the intervention The children were accompanied throughout the session by a caregiver who was oriented not to help the child in the execution of the Fig The childs nonverbal cues elicited by the CRI to look towards the therapist towards the robot point and self occlusion JIntellRobotSyst Content courtesy of Springer Nature terms of use apply Rights reservedFig Performance of the childs face analysis pipeline for the case study Face detection and recognition landmarks detection head pose and eye gaze estimation were executed tasks The exercise developed aimed to direct the attention of the child towards objects located in the room through stimuli such as look at point and speak The stimuli were generated first only by the therapist and later just by the robot Subjects Three children without confirmed ASD diagnosis but with evidence of risk factors and three typically developing TD children as the control group participated in the experiments All volunteers participated with their parents consent which were five boys ASD TD and one TD girl between months to months Each volunteer participated in one single session The goal was to analyze the basedline of the childs behavior and establish differences in the behavioral reaction between TD and ASDchildren for stimuli generated through CRI and leverage the novelty effect raised by the robot mediator ResultsandDiscussion The childs nonverbal cues elicited by the CRI can be o b s e r v e di nF i g Some examples of childrens behavior tagged to perform the behavioral coding are shown in the six pictures The tagged behaviors were to look towards an object towards the robot and towards the therapist to point and to respond to a prompt of both mediators and self occlusion Typical occlusion problem as occlusion by hair hands and the robot were detected The performance of video processing in the proof of concept session is reported in Fig In the case study sessions the childs face detection and recognition the Fig Evolution over time of the childs headneck rotation yaw rotation for a TD group JIntellRobotSyst Content courtesy of Springer Nature terms of use apply Rights reservedFig Evolution over time of the childs headneck rotation yaw rotation for a TD volunteer and VFOA estimation results landmarks detection head pose and eye gaze estimation for different viewpoints are shown in Fig The recognition process was able to detect all faces in the session successfully in most cases The childs head pose was captured throughout the session and analyzed automatically to estimate the evolution over time of childs head and the VFOA Along the session the childs neck rightleft rotation movement was predominant Yaw axis while the neck flexionextension Pitch axis and neck RL lateral flexion movements Roll axis remained approximately constant The Yaw rotation of the TD children group is reported in Fig The vertical light blue stripe indicates the intervention period with therapist mediator and the vertical light green stripe represents the period with robotmediator The continuous blue line represents the raw data recorded and the continuous red line describes the average data trend From the observation of the three plot the TD children started the intervention looking towards the robot evidently the robot was a naturalistic attention attractor Subsequently when the therapist begins the protocol explaining the tasks the children attention shifts towards the therapist The children remained this behavior until that the therapist introduced the robot mediator In this transition the childrens behaviors such asRJA and IJA toward the therapist were observed Once the therapist changed the mediation with the robot the children turned hisher attention to the robot and the objects in the room A more detailed analysis of one of the TD volunteers is shown in Fig The plot A shows the overall intervention session the plot B and plot C are a zoom of the period with therapist and robot mediator respectively The colors convention in the three plots of Fig describes the results generated by the automated estimation of VFOA From these scenarios some essential aspects already emerge In the therapistmediator interval the child responded to JA task using only one repetition for all prompt level The childs behavior of RJA was according to the protocol ie the child looked towards the therapist to wait for instructions rapidly the child searched in the target and next looked again toward the therapist Color sequence light blue yellow light blue orange light blue red This behavior was the same for all prompts In contrast with the robotmediator the child did not look toward the robot among indications at consecutive targets Color sequence light green yellow orange red orange yellow The above happened because in the protocol both mediators executed the instructions in the same order andJIntellRobotSyst Content courtesy of Springer Nature terms of use apply Rights reservedFig Evolution over time of the childs headneck rotation yaw rotation for a ASD group the child memorized the commands and the objects position until the robot mediator interval This fact did not affect the interventions aim as the robot mediator succeeded to elicit the childs behaviors of RJA and IJA In addition as highlighted in the plot A in Fig when the session finalized and the robot mediator said goodbye again RJA and IJA behaviors were perceived The pictures ad show these events first the child said goodbye towards the robot then he looked the therapist to confirm that the session ended and looked again towards the robot finally the child took the robots hand From the analysis of the three TD volunteers the same reported behaviors were perceived However the analysis of the children in the ASD group showed different behavior patterns concerning comfort visual contact and novelty stimulus effect during the sessions The evolution over time of the childs headneck rotation yaw rotation for an ASD group is shown in Fig On the one hand the three children in the ASD group maintained more visual contact with the robot compared to the therapist and exhibited more interest in the robot platform compared to the TD children However the performance of the children in the activities of JA did not improve significantly when the robot executed the prompt On the other hand the clinicians manifested that in all cases the first visual contact toward them occurred in the instant that the robot entered the scene and startedinteracting ie the ONO mediation elicited behaviors of IJA towards the therapist In addition the CwASD exhibited less discomfort regarding the session from the first moment when the robot initiated mediation in the room and in some cases when showed appearance of verbal and nonverbal prosocial behaviors These facts did not arise with the TD children because the first visual contact with the therapist occurred when they entered the room Additionally TD children showed the ability to divide the attention between the robot and the therapist from the beginning to the end of the intervention exhibiting comfort in every moment The behavior modulation of CwASD is observed in Fig Before the period with robotmediator the children exhibited discomfort unstable movements of their head and after of this period the head movement tended to be more stable The novelty of a robotmediator at diagnostic session can be analyzed as an additional stimulus of the CRI Accordingly in this case study the children of the ASD group showed more behavior modification attention and comfort produced by the robot interaction at the beginning of the CRI remaining until the end of the session On the other hand the children of the TD group responded to the novelty effect of the robot mediator from the time the child entered the room and saw the robot until the beginning of the therapist presentation For the above despite the noveltyJIntellRobotSyst Content courtesy of Springer Nature terms of use apply Rights reservedof the stimuli effect these did not seem to affect the social interaction between the TD children and the therapist and in contrast these stimuli seemed to enhance the CwASD social interaction with the therapist along the intervention These results are impressive since they show the potential of CRI intervention to systematically elicit differences between the pattern of behavior on TD and ASD children We identified RJA and IJA toward the therapist at the beginning of the intervention at the transition between therapist to robot mediator and at the end for all TD children In contrast we only identified IJA towards the therapist in the transition between mediators for ASD children This fact shows a clear difference of behavior pattern between CwASD and TD children which can be analyzed using a JA task protocol In fact these pattern differences can be used as evidence to improve the ASD diagnosis Conclusions This work presented a RobotAssisted tool to assist and enhance the traditional practice of ASD diagnosis The designed framework combines a vision system with the automated analysis of nonverbal cues in addition to a robotic platform both developed upon open source projects This research contributes to the stateoftheart with an innovative flexible and scalable architecture capable to automatically register events of joint attention and patterns of visual contact before and after of a robotbased mediation as well as the pattern of behavior related to comfort or discomfort along the ASD intervention In addition an artificial vision pipeline based on a multi camera approach was proposed The vision system performs face detection recognition and tracking landmark detection and tracking head pose gaze and estimation of visual focus of attention was proposed with its performance considered suitable for use into conventional ASD intervention At least one camera captured the childs face in each sample frame Furthermore the feedback information about the childs performance was successfully used to modulate the supervised behavior of ONO improving the performance of the CRI and the visual attention of the children Regarding the VFOA estimation the algorithm was able to estimate the target into the FoV in different situations recurrently Also the robot was able to react according to the estimation However the algorithm only failed when occlusion by the childs hands is generated On the other hand the occlusion by the therapist and the robot was compensated using the multicamera approach The childs face recognition system showed to be imperative to analyze the childs behavior in the clinical setup implemented in this work which required the caregivers attention in the roomDespite the limited number of children of this study preliminary results of this case study showed the feasibility of identifying and quantify differences in the patterns of behavior of TD children and CwASD elicited by the CRI intervention Through the proof of concept it is evidenced here the system ability to improve the traditional tools used in ASD diagnosis As future works it is recommended a study to replicate the protocol proposed in this paper with ten CwASD and ten TD children Another suggestion is to quantify other kinds of behaviors in addition to that assessed in this paper such as verbal utterance patterns physical and emotional engagement object or event preferences and gather more evidence to improve the assistance to therapists in ASD diagnosis processes Acknowledgements This work was supported by the Google Latin America Research Awards LARA program The first author scholar ship was supported in part by the Coordenac ao de Aperfeic oamento de Pessoal de N vel Superior Brasil CAPES Finance Code Disclosurestatement No potential conflict of interest was reported by the authors References Belpaeme T Baxter PE de Greeff J Kennedy J Read R Looije R Neerincx M Baroni I Zelati MC ChildRobot interaction perspectives and challenges In th International Conference ICSR pp Springer International Publishing Bristol Diehl JJ Schmitt LM Villano M Crowell CR The clinical use of robots for individuals with autism spectrum disorders A critical review Res Autism Spectr Disord Scassellati B Admoni H Maja M Robots for use in autism research Annu Rev Biomed Eng Pennisi P Tonacci A Tartarisco G Billeci L Ruta L Gangemi S Pioggia G Autism and social robotics A systematic review American Psychiatric Association DSM diagnostic classifica tion In Diagnostic and Statistical Manual of Mental Disorders American Psychiatric Association Eggebrecht AT Elison JT Feczko E Todorov A Wolff JJ Kandala S Adams CM Snyder AZ Lewis JD Estes AM Zwaigenbaum L Botteron KN McKinstry RC Constantino JN Evans A Hazlett HC Dager S Paterson SJ Schultz RT Styner MA Gerig G Das S Kostopoulos P Schlaggar BL Petersen SE Piven J Pruett JR Joint attention and brain functional connectivity in infants and toddlers Cerebral Cortex Steiner AM Goldsmith TR Snow AV Chawarska K Disorders in infants and toddlers J Autism Dev Disord Belpaeme T Baxter PE Read R Wood R Cuay ahuitl H Kiefer B Racioppa S KruijffKorbayov a I Athanasopoulos G Enescu V Looije R Neerincx M Demiris Y Ros Espinoza R Beck A Canamero L Hiolle A Lewis M Baroni I Nalin M Cosi P Paci G Tesser F Sommavilla G Humbert R Multimodal childrobot interaction building social bonds Journal of HumanRobot Interaction JIntellRobotSyst Content courtesy of Springer Nature terms of use apply Rights reserved Vanderborght B Simut R Saldien J Pop C Rusu AS Pintea S Lefeber D David DO Using the social robot probo as a social story telling agent for children with ASD Interact Stud Warren ZE Zheng Z Swanson AR Bekele E Zhang L Crittendon JA Weitlauf AF Sarkar N Can robotic interaction improve joint attention skills J Autism Dev Disord Wood LJ Dautenhahn K Lehmann H Robins B Rainer A Syrdal DS Robotmediated interviews Do robots pos sess advantages over human interviewers when talking to chil dren with special needs Lecture Notes in Computer Sci ence including subseries Lecture Notes in Artificial Intelli gence and Lecture Notes in Bioinformatics LNAI FeilSeifer D Mataric MJ bIA A control architecture for autonomous robotassisted behavior intervention for children with Autism Spectrum Disorders In ROMAN The th IEEE International Symposium on Robot and Human Interactive Communication pp Leo M Del Coco M Carcagn P Distante C Bernava M Pioggia G Palestra G Automatic emotion recognition in RobotChildren interaction for ASD treatment In Proceedings of the IEEE International Conference on Computer Vision Februc pp Esteban PG Baxter PE Belpaeme T Billing E Cai H Cao HL Coeckelbergh M Costescu C David D De Beir A Fang Y Ju Z Kennedy J Liu H Mazel A Pandey A Richardson K Senft E Thill S Van De Perre G Vanderborght B Vernon D Hui Y Ziemke T How to build a supervised autonomous system for RobotEnhanced therapy for children with autism spectrum disorder Paladyn Journal of Behavioral Robotics Pour AG Taheri A Alemi M Ali M HumanRobot facial expression reciprocal interaction platform case studies on children with autism Int J Soc Robot Feng Y Jia Q Wei W A control architecture of Robot Assisted intervention for children with autism spectrum disorders J Robot Bekele E Crittendon JA Swanson A Sarkar N Warren ZE Pilot clinical application of an adaptive robotic system for young children with autism Autism The International Journal of Research and Practice Huijnen CAGJ Lexis MAS Jansens R de Witte LP Mapping robots to therapy and educational objectives for children with autism spectrum disorder J Autism Dev Disord ArestiBartolome N Begonya GZ Technologies as support tools for persons with autistic spectrum disorder s systematic review Int J Environ Res Public Health Boucenna S Narzisi A Tilmont E Muratori F Pioggia G Cohen D Mohamed C Interactive technologies for autistic children a review Cogn Comput Grynszpan O Patrice L Weiss T PerezDiaz F Gal E Innovative technologybased interventions for autism spectrum disorders a metaanalysis Autism Rehg JM Rozga A Abowd GD Goodwin MS Behavioral imaging and autism IEEE Pervasive Comput Cabibihan JJ Javed H Ang M Aljunied SM Why robots a survey on the roles and benefits of social robots in the therapy of children with autism Int J Soc Robot Sartorato F Przybylowski L Sarko DK Improving therapeu tic outcomes in autism spectrum disorders enhancing social com munication and sensory processing through the use of interactive robots J Psychiatr Res Chong E Chanda K Ye Z Southerland A Ruiz N Jones RM Rozga A Rehg JM Detecting gaze towards eyes in natural social interactions and its use in child assessment Proc ACM Interact Mob Wearable Ubiquitous Technol Ness SL Manyakov NV Bangerter A Lewin D Jagannatha S Boice M Skalkin A Dawson G Janvier Y M Goodwin MS Hendren R Leventhal B Shic F Cioccia W Gahan P JAKE Multimodal data capture system Insights from an observational study of autism spectrum disorder Frontiers in Neuroscience SEP Rehg JM Abowd GD Rozga A Romero M Clements MA Sclaroff S Essa I Ousley OY Li Y Kim C Rao H Kim JC Lo Presti L Zhang J Lantsman D Bidwell J Ye Z Decoding childrens social behavior In IEEE Conference on Computer Vision and Pattern Recognition pp Adamo F Palestra G Crifaci G Pennisi P Pioggia G Ruta L Leo M Distante C Cazzato D Nonintrusive and calibration free visual exploration analysis in children with autism spectrum disorder In Computational Vision and Medical Image Processing V Proceedings of th Eccomas Thematic Conference on Computational Vision and Medical Image Processing VipIMAGE pp Michaud F Salter T Duquette A Mercier H Lauria M Larouche H Larose F Assistive technologies and ChildRobot interaction American Association for Artificial Intelligence ii Duquette A Michaud F Mercier H Exploring the use of a mobile robot as an imitation agent with children with low functioning autism Auton Robot Simut RE Vanderfaeillie J Peca A Van de Perre G Bram V Children with autism spectrum disorders make a fruit salad with probo the social robot an interaction study J Autism Dev Disord Bekele E Lahiri U Swanson AR Crittendon JA Warren ZE Nilanjan S A step towards developing adaptive robot mediated intervention architecture ARIA for children with autism IEEE Trans Neural Syst Rehabil Eng Zheng Z Zhang L Bekele E Swanson A Crittendon JA Warren ZE Sarkar N Impact of robotmediated interaction system on joint attention skills for children with autism In IEEE International Conference on Rehabilitation Robotics Anzalone SM Tilmont E Boucenna S Xavier J Jouen AL Bodeau N Maharatna K Chetouani M Cohen D How children with autism spectrum disorder behave and explore the dimensional spatial D time environment during a joint attention induction task with a robot Res Autism Spectr Disord Chevalier P Martin JC Isableu B Bazile C Iacob DO Adriana T Joint attention using humanrobot interaction impact of sensory preferences of children with autism In th IEEE International Symposium on Robot and Human Interactive Communication ROMAN pp Lemaignan S Garcia F Jacq A Dillenbourg P From real time attention assessment to withmeness in humanrobot interaction In ACMIEEE International Conference on Human Robot Interaction April pp Del Coco M Leo M Carcagni P Fama F Spadaro L Ruta L Pioggia G Distante C Study of mechanisms ofJIntellRobotSyst Content courtesy of Springer Nature terms of use apply Rights reservedsocial interaction stimulation in autism spectrum disorder by assisted humanoid robot IEEE Transactions on Cognitive and Developmental Systems c Palestra G Varni G Chetouani M Esposito F A multimodal and multilevel system for robotics treatment of autism in children In Proceedings of the International Workshop on Social Learning and Multimodal Interaction for Designing Artificial Agents DAA pp ACM Press New York Quigley M Gerkey B Conley K Faust J Foote T Leibs J Berger E Wheeler R Ng A ROS an opensource robot operating system In ICRA workshop on open source software number pp Vandevelde C Saldien J Ciocci C Vanderborght B The use of social robot ono in robot assisted therapy In International Conference on Social Robotics Proceedings m Dautenhahn K A paradigm shift in artificial intelligence why social intelligence matters in the design and development of robots with humanlike intelligence Years of Artificial Intelligence pp Ekman P Friesen W Facial Action Coding System Consulting Psychologists Press King DE Dlibml a machine learning toolkit J Mach Learn Res Baltru saitis T Robinson P Morency LP OpenFace an open source facial behavior analysis toolkit IEEE Winter Conference on Applications of Computer Vision King DE MaxMargin Object Detection He K Zhang X Ren S Jian S Deep residual learning for image recognition In IEEE Conference on Computer Vision and Pattern Recognition CVPR pp IEEE Baltru saitis T Robinson P Morency LP Constrained local neural fields for robust facial landmark detection in the wild In Proceedings of the IEEE International Conference on Computer Vision pp Cristinacce D Cootes TF Feature detection and tracking with constrained local models In Proceedings of the British Machine Vision Conference pp Saragih JM Lucey S Cohn JF Deformable model fitting by regularized landmark meanshift Int J Comput Vis Baltru saitis T Robinson P Morency LP D constrained local model for rigid and nonrigid facial tracking In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition pp Belhumeur PN Jacobs DW Kriegman DJ Neeraj K Localizing parts of faces using a consensus of exemplars IEEE Trans Pattern Anal Mach Intell Le V Brandt J Lin Z Bourdev L Huang TS Interactive Facial Feature Localization pp Springer Berlin Jorstad A Dementhon D Jeng Wang I Burlina P Distributed consensus on camera pose IEEE Trans Image Process Ba SO Odobez JM MultiPerson visual focus of attention from head pose and meeting contextual cues IEEE Trans Pattern Anal Mach Intell August Sheikhi S JeanMarc O Combining dynamic head pose gaze mapping with the robot conversational state for attention recognition in humanrobot interactions Pattern Recogn Lett Publishers Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations AndresARam rezDuque received his bachelors degree in Mecha tronics Engineering from the Universidad Nacional de Colombia Bogot a Colombia in and his Industrial Automation Master degree from the Universidad Nacional de Colombia Bogot a Colom bia in He is currently working toward a PhD degree in the Assistive Technology Center Federal University of Esp rito Santo Vitoria Brazil He won a Google Latin America Research Award His current research interests include ChildRobot interaction cloud parallel computing high performance computing smart environments and serious games applied to Children with development impairments Anselmo FrizeraNeto received his bachelors degree in Electrical Engineering from the Federal University of Esp rito Santo UFES in Brazil and his doctorate in Electronics at the University of Alcal a Spain From to he was a researcher of the Bioengineering Group of the Consejo Superior de Investigaciones Cient ficas Spain where he carried out research related to his doctoral thesis He is currently a permanent professor and adjunct coordinator of the Graduate Program in Electrical Engineering at UFES He has authored or coauthored more than papers in scientific journals books and conferences in the fields of electrical and biomedical engineering He has conducted or codirected masters and doctoral theses in research institutions from Brazil Argentina Italy and Portugal His research is aimed at rehabilitation robotics the development of advanced strategies of humanrobot interaction and the conception of sensors and measurement technologies with applications in different fields of electrical and biomedical engineering Along with Andr es Ram rezDuque he won a Google Latin America Research Award Teodiano Freire Bastos received his BSc degree in Electrical Engineering from Universidade Federal do Esp rito Santo Vit oria Brazil in his Specialist degree in Automation degree from Instituto de Autom atica Industrial Madrid Spain in and his PhD degree in Physical Science Electricity and Electronics from Universidad Complutense de Madrid Spain in He made two postdocs one at the University of Alcal a Spain and another at RMIT University Australia He is currently a full professor at Universidade Federal do Esp rito Santo Vit oria Brazil teaching and doing research at the Postgraduate Program of Electrical Enginneering Postgraduate Program of Biotechnology and RENORBIO PhD Program His current research interests are signal processing rehabilitation robotics and assistive technology for people with disabilitiesJIntellRobotSyst Content courtesy of Springer Nature terms of use apply Rights reserved Terms and Conditions Springer Nature journal content brought to you courtesy of Springer Nature Customer Service Center GmbH Springer Nature Springer Nature supports a reasonable amount of sharing of research papers by authors subscribers and authorised users Users for small scale personal noncommercial use provided that all copyright trade and service marks and other proprietary notices are maintained By accessing sharing receiving or otherwise using the Springer Nature journal content you agree to these terms of use Terms For these purposes Springer Nature considers academic use by researchers and students to be noncommercial These Terms are supplementary and will apply in addition to any applicable website terms and conditions a relevant site licence or a personal subscription These Terms will prevail over any conflict or ambiguity with regards to the relevant terms a site licence or a personal subscription to the extent of the conflict or ambiguity only For Creative Commonslicensed articles the terms of the Creative Commons license used will apply We collect and use personal data to provide access to the Springer Nature journal content We may also use these personal data internally within ResearchGate and Springer Nature and as agreed share it in an anonymised way for purposes of tracking analysis and reporting We will not otherwise disclose your personal data outside the ResearchGate or the Springer Nature group of companies unless we have your permission as detailed in the Privacy Policy While Users may use the Springer Nature journal content for small scale personal noncommercial use it is important to note that Users may not use such content for the purpose of providing other users with access on a regular or large scale basis or as a means to circumvent access control use such content where to do so would be considered a criminal or statutory offence in any jurisdiction or gives rise to civil liability or is otherwise unlawful falsely or misleadingly imply or suggest endorsement approval sponsorship or association unless explicitly agreed to by Springer Nature in writing use bots or other automated methods to access the content or redirect messages override any security feature or exclusionary protocol or share the content in order to create substitute for Springer Nature products or services or a systematic database of Springer Nature journal content In line with the restriction against commercial use Springer Nature does not permit the creation of a product or service that creates revenue royalties rent or income from our content or its inclusion as part of a paid for service or for other commercial gain Springer Nature journal content cannot be used for interlibrary loans and librarians may not upload Springer Nature journal content on a large scale into their or any other institutional repository These terms of use are reviewed regularly and may be amended at any time Springer Nature is not obligated to publish any information or content on this website and may remove it or features or functionality at our sole discretion at any time with or without notice Springer Nature may revoke this licence to you at any time and remove access to any copies of the Springer Nature journal content which have been saved To the fullest extent permitted by law Springer Nature makes no warranties representations or guarantees to Users either express or implied with respect to the Springer nature journal content and all parties disclaim and waive any implied warranties or warranties imposed by law including merchantability or fitness for any particular purpose Please note that these rights do not automatically extend to content data or other material published by Springer Nature that may be licensed from third parties If you would like to use or distribute our Springer Nature journal content to a wider audience or on a regular basis or in any other manner not expressly permitted by these Terms please contact Springer Nature at